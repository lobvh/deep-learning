{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**// Importing necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding review\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "                        [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join(\n",
    "                        [reverse_word_index.get(i-3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of decoding:** \n",
    "\n",
    "    By default when you call imdb.load_data it will set some parameters by default if you don't define them explicitly. That means, by default, indexing starts with 3. That is why you use i-3. get_word_index will contain all the words in usuall order and the indexing starts AT ZERO. When indexing starts with 3 it just means that you leave three blank spaces _ _ _ which represent places for padding, start of sentence and unknown (indexing it 0, 1 and 2 respectively). Hence, third word in train_data[0] (i = 3) equals to first a.k.a. (i-3)th word in reverse_word_index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the integer sequences into a binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of function:\n",
    " - Create an matrix whece each row represents one sequence from train_data. \n",
    " - Size of the row is 10000 because we are restricted to 10000 most frequent words.\n",
    " - Make the default values zero. \n",
    " - For each such sequence on each number where the word occurs (for example 1, 14 16) put 1 \n",
    " - Do this for each sentence in the train_data and you will get one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = np.zeros((len(train_data), 10000))\n",
    "results[0,[2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(train_data):\n",
    "    if i<1:\n",
    "        print(i, sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using custom losses and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.01),\n",
    "             loss = losses.binary_crossentropy,\n",
    "             metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting aside a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ovdje je on uzimo model.compile() po defaultu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 145us/step - loss: 0.5304 - binary_accuracy: 0.7439 - val_loss: 0.4015 - val_binary_accuracy: 0.8293\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.2658 - binary_accuracy: 0.8919 - val_loss: 0.3575 - val_binary_accuracy: 0.8406\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.1998 - binary_accuracy: 0.9251 - val_loss: 0.2970 - val_binary_accuracy: 0.8894\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.1404 - binary_accuracy: 0.9447 - val_loss: 0.3576 - val_binary_accuracy: 0.8793\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.1093 - binary_accuracy: 0.9572 - val_loss: 0.4166 - val_binary_accuracy: 0.8824\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 0.0903 - binary_accuracy: 0.9684 - val_loss: 0.4442 - val_binary_accuracy: 0.8833\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.0717 - binary_accuracy: 0.9739 - val_loss: 0.5111 - val_binary_accuracy: 0.8793\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.1206 - binary_accuracy: 0.9753 - val_loss: 0.5064 - val_binary_accuracy: 0.8788\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.0108 - binary_accuracy: 0.9969 - val_loss: 0.6267 - val_binary_accuracy: 0.8791\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 88us/step - loss: 0.0747 - binary_accuracy: 0.9822 - val_loss: 0.6089 - val_binary_accuracy: 0.8774\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 73us/step - loss: 0.0043 - binary_accuracy: 0.9988 - val_loss: 0.7530 - val_binary_accuracy: 0.8765\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 0.0023 - binary_accuracy: 0.9991 - val_loss: 0.8824 - val_binary_accuracy: 0.8738\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.1514 - binary_accuracy: 0.9819 - val_loss: 0.8757 - val_binary_accuracy: 0.8728\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.0010 - binary_accuracy: 0.9997 - val_loss: 0.9288 - val_binary_accuracy: 0.8743\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 73us/step - loss: 5.6173e-04 - binary_accuracy: 0.9999 - val_loss: 0.9935 - val_binary_accuracy: 0.8733\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 74us/step - loss: 3.1681e-04 - binary_accuracy: 0.9999 - val_loss: 1.1170 - val_binary_accuracy: 0.8726\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 9.9624e-05 - binary_accuracy: 1.0000 - val_loss: 1.2670 - val_binary_accuracy: 0.8730\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 81us/step - loss: 0.2134 - binary_accuracy: 0.9787 - val_loss: 1.2746 - val_binary_accuracy: 0.8622\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 2.5278e-04 - binary_accuracy: 1.0000 - val_loss: 1.3109 - val_binary_accuracy: 0.8655\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 3.7333e-05 - binary_accuracy: 1.0000 - val_loss: 1.3288 - val_binary_accuracy: 0.8667\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                  partial_y_train,\n",
    "                  epochs = 20,\n",
    "                  batch_size = 512,\n",
    "                  validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting the training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kad stisnes slovo H u markwodn modu dobijes nesto zanimljivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss'] \n",
    "binary_accuracy_values = history_dict['binary_accuracy']\n",
    "\n",
    "epochs = range(1, len(binary_accuracy_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, 'b', label = \"Validation loss\")\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
