{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first time using Keras as a way of training ML/DL models. Here we need to classify IMDB comments as positive, or negative.\n",
    "We will use the very basics, and as time progress and me gains more knowledge so this model will improve. Hopefully, after this one, I will be able to apply my knowledge on my Titanic dataset project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#For making ticks integer values\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing files and spliting it on training and test data.** \n",
    "\n",
    "This is probably already preprocessed, and all we need to do is use tuples to unpack it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding review\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "                        [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join(\n",
    "                        [reverse_word_index.get(i-3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**//Explanation of decoding:** \n",
    "\n",
    "   By default when you call `imdb.load_data`, if you don't define them explicitly it will set some parameters implicitly. That means, by default, indexing in  starts with `3`. That is why you use `i-3`. `get_word_index` will contain all the words in usuall order and the indexing starts AT ZERO. When indexing starts with 3 it just means that you leave three blank spaces `_ _ _` which represent places for `padding`, `start of sentence` and `unknown` (indexing it 0, 1 and 2 respectively). Hence, third word in `train_data[0]` (`i = 3`) equals to first a.k.a. `(i-3)`th word in `reverse_word_index`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the integer sequences into a binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of function:\n",
    " - Create a matrix where each row represents one sequence from `train_data`. \n",
    " - Size of the row is 10000 because we are restricted to 10000 most frequent words.\n",
    " - Make the default values zero. \n",
    " - For each such sequence on each number where the word occurs (for example 1, 14 16) put 1 \n",
    " - Do this for each sentence in the `train_data` and you will get one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = np.zeros((len(train_data), 10000))\n",
    "results[0,[2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "for i, sequence in enumerate(train_data):\n",
    "    if i<1:\n",
    "        print(i, sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first model we will use, and I will play with the number of layers, as well as number of hidden units to see which ones give better predictions. Learning rate aswell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using custom losses and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.01),\n",
    "             loss = losses.binary_crossentropy,\n",
    "             metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting aside a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set of 10 000 validation sentences \n",
    "x_val = x_train[:10000]\n",
    "\n",
    "#... and the rest is training set\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "#Set of 10 000 labels of those sentences\n",
    "y_val = y_train[:10000]\n",
    "#... and the rest is training set labels\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 129us/step - loss: 0.5256 - binary_accuracy: 0.7753 - val_loss: 0.2994 - val_binary_accuracy: 0.8828\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.2721 - binary_accuracy: 0.8899 - val_loss: 0.2883 - val_binary_accuracy: 0.8814\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 64us/step - loss: 0.1876 - binary_accuracy: 0.9277 - val_loss: 0.3219 - val_binary_accuracy: 0.8798\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 64us/step - loss: 0.1516 - binary_accuracy: 0.9429 - val_loss: 0.3244 - val_binary_accuracy: 0.8816\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 64us/step - loss: 0.1102 - binary_accuracy: 0.9615 - val_loss: 0.5957 - val_binary_accuracy: 0.8020\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 64us/step - loss: 0.0879 - binary_accuracy: 0.9669 - val_loss: 0.4376 - val_binary_accuracy: 0.8786\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 65us/step - loss: 0.0777 - binary_accuracy: 0.9722 - val_loss: 0.5307 - val_binary_accuracy: 0.8786\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.0761 - binary_accuracy: 0.9785 - val_loss: 0.5163 - val_binary_accuracy: 0.8752\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.0123 - binary_accuracy: 0.9959 - val_loss: 0.7265 - val_binary_accuracy: 0.8720\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.1180 - binary_accuracy: 0.9802 - val_loss: 0.6764 - val_binary_accuracy: 0.8728\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.0057 - binary_accuracy: 0.9982 - val_loss: 0.8638 - val_binary_accuracy: 0.8731\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.0040 - binary_accuracy: 0.9981 - val_loss: 0.9778 - val_binary_accuracy: 0.8704\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.1333 - binary_accuracy: 0.9813 - val_loss: 0.8790 - val_binary_accuracy: 0.8700\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.0035 - binary_accuracy: 0.9988 - val_loss: 1.0143 - val_binary_accuracy: 0.8702\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.0027 - binary_accuracy: 0.9987 - val_loss: 1.1212 - val_binary_accuracy: 0.8691\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.0026 - binary_accuracy: 0.9987 - val_loss: 2.3779 - val_binary_accuracy: 0.7807\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.1569 - binary_accuracy: 0.9836 - val_loss: 1.1156 - val_binary_accuracy: 0.8696\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.0022 - binary_accuracy: 0.9992 - val_loss: 1.2371 - val_binary_accuracy: 0.8678\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.0018 - binary_accuracy: 0.9993 - val_loss: 1.3664 - val_binary_accuracy: 0.8661\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 68us/step - loss: 0.1160 - binary_accuracy: 0.9853 - val_loss: 1.1271 - val_binary_accuracy: 0.8682\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                  partial_y_train,\n",
    "                  epochs = 20,\n",
    "                  batch_size = 512,\n",
    "                  validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting the training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kad stisnes slovo H u markdown modu dobijes nesto zanimljivo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "#Making ticks integer valued\n",
    "ax = plt.figure().gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, 'b', label = \"Validation loss\")\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a common pitfal in training ML/DL, that is overfitting. After second epoch our training loss decreases (expected for gradient optimization based networks!), BUT our validation loss increases. That is, after third epoch the nework is 'memorizing' training samples, and can't generalize good. We see that training_loss == validation_loss at second epoch, but I will re-train the network on my whole training dataset by 3 epochs. The accuracy also plateous around 85ish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc_values = history_dict['binary_accuracy']\n",
    "val_acc_values = history_dict['val_binary_accuracy']\n",
    "\n",
    "#Making ticks integer valued\n",
    "ax = plt.figure().gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.01),\n",
    "             loss = losses.binary_crossentropy,\n",
    "             metrics = [metrics.binary_accuracy])\n",
    "\n",
    "history_3 = model.fit(x_train,\n",
    "                  y_train,\n",
    "                  epochs = 3,\n",
    "                  batch_size = 512)\n",
    "\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = results\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a trained network to generate predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's explore the possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding more complexity to a model it's said that the network is prone to overfitting. I think for checking that assumption we would need to try all sorts of combinations of tuning hyperparameters which here are layers, hidden units, activation functions, optimizer, choosing the loss function, batch_size etc. Will train one more just for a sake of practise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 layers, 32 hidden units\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "          \n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 0.01),\n",
    "             loss = losses.binary_crossentropy,\n",
    "             metrics = [metrics.binary_accuracy])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                  partial_y_train,\n",
    "                  epochs = 20,\n",
    "                  batch_size = 512,\n",
    "                  validation_data = (x_val, y_val)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### // Training & validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss'] \n",
    "\n",
    "binary_accuracy_values = history_dict['binary_accuracy']\n",
    "epochs = range(1, len(binary_accuracy_values) + 1)\n",
    "\n",
    "#Making ticks integer valued\n",
    "ax = plt.figure().gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, 'b', label = \"Validation loss\")\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### // Training & validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc_values = history_dict['binary_accuracy']\n",
    "val_acc_values = history_dict['val_binary_accuracy']\n",
    "\n",
    "#Making ticks integer valued\n",
    "ax = plt.figure().gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
