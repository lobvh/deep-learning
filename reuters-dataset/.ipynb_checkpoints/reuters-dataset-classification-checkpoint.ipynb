{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini-project we have a problem of single label, multiclass classification. The dataset we are using is 'Reuters dataset', which is already included in the Keras package. Reuters dataset is a set of short newswires and their topics, published by Reuters in 1986. Each newswire belongs to one of the 46 different topic. \n",
    "It's a multiclass problem, because there are 46 different topics, yet single label since each data point has one-to-one corespodence with one of the 46 topics. \n",
    "\n",
    "This problem is 'almost the same' with the binary classification one, and the differences are as follows:\n",
    "\n",
    "* What sigmoid is for binary classification, soft-max is for multiclass classification\n",
    "* Since the dimension of output space is 46 we should 'not' include information bottleneck. That is, previous layers have less dimensions (aka units) than the (final) output layer/s, and hence compressing too much information which could help to make separation hyperplanes of 46 classes is a bad juju!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing the dataset from Keras dataset library\n",
    "from keras.datasets import reuters\n",
    "\n",
    "#Importing libraries for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Importing libraries for everything that deals with modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.metrics import accuracy\n",
    "\n",
    "#For making ticks integer values\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing training and test part of the set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are gonna stick to first 10000 most frequently used words to make learning feasible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982,), (2246,), (8982,), (2246,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checks to see if we imported data properly\n",
    "\n",
    "train_data.shape, test_data.shape, train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8982 newswires in training, and 2246 in the test set. Close to 80%/20% split. Now we will see one of the decoded newswires and explore all the unique topics that those newswires are associated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding first newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = reuters.get_word_index() # {'word':'frequency_of_word'}\n",
    "reverse_word_index = dict([(value, key)\n",
    "                             for (key, value) in word_index.items()]) # {'frequency_of_word':'word'}\n",
    "\n",
    "#Let's decode the first newswire in our training dataset\n",
    "decoded_review_1 = ' '.join([reverse_word_index.get(i-3, '?')\n",
    "                              for i in train_data[0]])\n",
    "decoded_review_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels), np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive approach forgeting that most frequent words are for the newswires, not for unique labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_topics = []\n",
    "for idx in np.unique(train_labels):\n",
    "    list_of_topics.append(reverse_word_index.get(idx))\n",
    "    \n",
    "#list_of_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the orginal ones scraped from the web: \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_topics = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
    "                  'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
    "                  'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
    "                  'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
    "                  'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/SteffenBauer/KerasTools/blob/master/KerasTools/datasets/decode.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in the IMDB example input data, as well as labels that we feed the network with must be vectorized, and by default we will use one-hot-encoding. But this time for the labels we could either choose to use one-hot-encoding IF we are using categorical_cross_entropy as our loss function to minimize, or we could leave them as numpy arrays but in that case we use sparse_categorical_cross_entropy. \n",
    "There is no difference between using each one of them, both loss functions are treating labels the same but one expects OHE, other one is 'okay' with the integer labels. \n",
    "\n",
    "PS I saw that this concept of converting integers to one-hot-encoding is actually 'bag of words' kinda thing. \n",
    "\n",
    "\"If there is a word in this sequence of 10000 most frequent words on that index included write 1, else 0.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimensions = 10000):\n",
    "    results = np.zeros((len(sequences), dimensions))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing training and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_labels(labels, dimensions = 46):\n",
    "    results = np.zeros((len(labels), dimensions))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i,label] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = vectorize_labels(train_labels)\n",
    "y_test = vectorize_labels(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we explained in the summary part we are gonna use the architeture such that layers up to the last one have hidden units that are bigger or equal then the last one. I forgot to explain the softmax activation, which is used as an output when we have multiclass problems. One can think of softmax as 'probability distribution' since for each number in vector [a, b, c ... d] it returns it's probability [p(a), p(b), p(c), ... p(d)]. Seen that vector as encoding each probability of each term of a vector we can think of it as a distribution of probabilities. \n",
    "\n",
    "I think the way I presented the model here might be cumbersome, so in the next project I will import libraries to make my code more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(46, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = RMSprop(lr = 0.01),\n",
    "             loss = categorical_crossentropy,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the training data into training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set apart 1000 samples in the training data as our validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation set\n",
    "x_val = x_train[:1000]\n",
    "\n",
    "#Partial training set\n",
    "partial_x_train = x_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the rest of the samples are those that we are gonna train on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation labels\n",
    "y_val = y_train[:1000]\n",
    "\n",
    "#Partial training set labels\n",
    "partial_y_train = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
